# -*- coding: utf-8 -*-
"""Fake News Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lb6-3eK83Hq5tfXWfMrYDn3OCmkH21qI

# The aim of the project is to build a fake news classifier using Natural Language Processing.
"""

print("Loading libraries")

import re
import os
import nlp_utils
import pandas as pd
import time
import seaborn as sns
import re
import string

# import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

print("current path: ", os.getcwdb())
 
print(" Reading the data ")

df=pd.read_csv('C:/Users/amith/Documents/fake_news_proj/train.csv')

print(df.info())
## There are 20800 rows and 5 columns as seen above

# df['title']
# ## Title contains the headline of the news

# df['text']
# ## text contains the information regarding the headline.

# df['label'].value_counts()
# ## There are 10413 'ones' and 10387 'zeroes' in the dataframe

print("NULLS CHECK 1")
df.isnull().sum()
## There are few null values present in the dataframe

print("Drop the Nulls")
df=df.dropna()
## The null values are removed using the dropna function

print("NULLS CHECK 2 ")
df.isnull().sum()
## As seen below there are no null values present in the dataframe now.

df.reset_index(inplace=True)
## As we can see in the output, the Series. reset_index() function has reset the index of the given Series.


# """## select 500 observations"""

# df=df.head(500)


print(" Text cleaning ")

# remove all numbers with letters attached to them
alphanumeric = lambda x: re.sub(r'\w*\d\w*', ' ', x)

# .lower() - convert all strings to lowercase
punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())

# Remove all '\n' in the string and replace it with a space
remove_n = lambda x: re.sub("\n", " ", x)

# Remove all non-ascii characters
remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)

print("Apply all the lambda functions")
# Apply all the lambda functions wrote previously through .map on the comments column
df['text'] = df['text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)


print(" Removing stop words and stemming the text ")

#### In natural language processing, useless words (data), are referred to as stop words. ... Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.

#### The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.

# --- Start Timer ---
start_time = time.time()

# --- Your Text Preprocessing Code ---

# Initialize SnowballStemmer
ps = SnowballStemmer(language='english')

corpus = []
for i in range(0, len(df)):
    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])
    review = review.lower()
    review = review.split()

    # Ensure stopwords are downloaded if you haven't already
    # import nltk
    # nltk.download('stopwords')

    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

# --- End Timer ---
end_time = time.time()
elapsed_time = end_time - start_time

print(f"Text preprocessing (with SnowballStemmer) completed in {elapsed_time:.4f} seconds.")

print("Splitting the dataframe")

Y=df['label']
## We select the label column as Y

print("Making train and test data")

X_train, X_test, Y_train, Y_test = train_test_split(df['text'], Y, test_size=0.30, random_state=40)
## We have split the data into 70 percent train and 30 percent test

print("Applying tfidf to the data set")
tfidf_vect = TfidfVectorizer(stop_words = 'english',max_df=0.7)
tfidf_train = tfidf_vect.fit_transform(X_train)
tfidf_test = tfidf_vect.transform(X_test)

print(tfidf_test)

# Get the feature names of `tfidf_vectorizer`
print(tfidf_vect.get_feature_names_out()[-10:])

print("Count vectorizer")

count_vect = CountVectorizer(stop_words = 'english')
count_train = count_vect.fit_transform(X_train.values)
count_test = count_vect.transform(X_test.values)

print(count_test)

# Get the feature names of `count_vectorizer`
print(count_vect.get_feature_names_out()[0:10])

print("Machine learning Models training, validation, saving the model")

print("1. Naive Bayes model")

print(" -- TF-Idf vectorized -- ")

#Applying Naive Bayes
clf = MultinomialNB()
clf.fit(tfidf_train, Y_train)
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(Y_test, pred)
print("accuracy:   %0.3f" % score)
cm = metrics.confusion_matrix(Y_test, pred)
print(cm)

print('Wrong predictions out of total')
print((Y_test !=pred).sum(),'/',((Y_test == pred).sum()+(Y_test != pred).sum()))
print('Percentage accuracy: ',100*accuracy_score(Y_test,pred))

print('Saving pickle file')

model_filename_pickle = 'multinomial_nb_tfidf_model.pkl'
with open(model_filename_pickle, 'wb') as file:
    pickle.dump(clf, file)
print(f"Model saved successfully to {model_filename_pickle}")
print("Model path: ",os.getcwdb())

## Plotting confusion matrix for TF-Idf vectorizer

sns.heatmap(cm, cmap="plasma", annot=True)

print(" -- Count vectorized -- ")

#Applying Naive Bayes
clf = MultinomialNB()
clf.fit(count_train, Y_train)
pred1 = clf.predict(count_test)
score = metrics.accuracy_score(Y_test, pred1)
print("accuracy:   %0.3f" % score)
cm2 = metrics.confusion_matrix(Y_test, pred1)
print(cm2)

print('Wrong predictions out of total')
print((Y_test !=pred1).sum(),'/',((Y_test == pred1).sum()+(Y_test != pred1).sum()))
print('Percentage accuracy: ',100*accuracy_score(Y_test,pred1))

print('Saving pickle file')

model_filename_pickle = 'multinomial_nb_cv_model.pkl'
with open(model_filename_pickle, 'wb') as file:
    pickle.dump(clf, file)
print(f"Model saved successfully to {model_filename_pickle}")
print("Model path: ",os.getcwdb())

## Plotting confusion matrix for Count vectorizer.

sns.heatmap(cm2, cmap="plasma", annot=True)

print("2. Random Forest Model")

print("TF-Idf Vectorized")

RF=RandomForestClassifier().fit(tfidf_train,Y_train)
#predict on train
train_preds2 = RF.predict(tfidf_train)
#accuracy on train
print("Model accuracy on train is: ", accuracy_score(Y_train, train_preds2))

#predict on test
test_preds2 = RF.predict(tfidf_test)
#accuracy on test
print("Model accuracy on test is: ", accuracy_score(Y_test, test_preds2))
print('-'*50)

#Confusion matrix
print("confusion_matrix train is: ", metrics.confusion_matrix(Y_train, train_preds2))
print("confusion_matrix test is: ", metrics.confusion_matrix(Y_test, test_preds2))
print('Wrong predictions out of total')
print('-'*50)

# Wrong Predictions made.
print((Y_test !=test_preds2).sum(),'/',((Y_test == test_preds2).sum()+(Y_test != test_preds2).sum()))
print('-'*50)

print("# --- Code to Save the RandomForestClassifier Model using pickle ---")
model_filename_nb = 'multinomial_rf_tfidf_model.pkl'
with open(model_filename_nb, 'wb') as file: # 'wb' means write binary
    pickle.dump(clf, file)
print(f"\nMultinomialNB model saved successfully to {model_filename_nb}")

# # --- How to Load the MultinomialNB Model Later (for demonstration) ---
# print("\n--- Demonstrating how to load the MultinomialNB model ---")
# with open(model_filename_nb, 'rb') as file: # 'rb' means read binary
#     loaded_nb_model = pickle.load(file)
# print(f"MultinomialNB model loaded successfully from {model_filename_nb}")

"""### Count Vectorized"""

RF=RandomForestClassifier().fit(count_train,Y_train)
#predict on train
train_preds3 = RF.predict(count_train)
#accuracy on train
print("Model accuracy on train is: ", accuracy_score(Y_train, train_preds3))

#predict on test
test_preds3 = RF.predict(count_test)
#accuracy on test
print("Model accuracy on test is: ", accuracy_score(Y_test, test_preds3))
print('-'*50)



#Confusion matrix
print("confusion_matrix train is: ", metrics.confusion_matrix(Y_train, train_preds3))
print("confusion_matrix test is: ", metrics.confusion_matrix(Y_test, test_preds3))
print('Wrong predictions out of total')
print('-'*50)

# Wrong Predictions made.
print((Y_test !=test_preds3).sum(),'/',((Y_test == test_preds3).sum()+(Y_test != test_preds3).sum()))
print('-'*50)

print("# --- Code to Save the RandomForestClassifier Model using pickle ---")
model_filename_nb = 'multinomial_rf_cv_model.pkl'
with open(model_filename_nb, 'wb') as file: # 'wb' means write binary
    pickle.dump(clf, file)
print(f"\nMultinomialNB model saved successfully to {model_filename_nb}")

print("K-Nearest Neighbour")

#fit the model on train data
KNN = KNeighborsClassifier().fit(tfidf_train,Y_train)
#predict on train
train_preds4 = KNN.predict(tfidf_train)
#accuracy on train
print("Model accuracy on train is: ", accuracy_score(Y_train, train_preds4))

#predict on test
test_preds4 = KNN.predict(tfidf_test)
#accuracy on test
print("Model accuracy on test is: ", accuracy_score(Y_test, test_preds4))
print('-'*50)

#Confusion matrix
print("confusion_matrix train is: ", metrics.confusion_matrix(Y_train, train_preds4))
print("confusion_matrix test is: ", metrics.confusion_matrix(Y_test, test_preds4))
print('Wrong predictions out of total')
print('-'*50)

# Wrong Predictions made.
print((Y_test !=test_preds4).sum(),'/',((Y_test == test_preds4).sum()+(Y_test != test_preds4).sum()))

print('-'*50)

print("# --- Code to Save the KNeighborsClassifier Model using pickle ---")
model_filename_nb = 'multinomial_KN_tfidf_model.pkl'
with open(model_filename_nb, 'wb') as file: # 'wb' means write binary
    pickle.dump(clf, file)
print(f"\nMultinomialNB model saved successfully to {model_filename_nb}")

#fit the model on train data
KNN = KNeighborsClassifier().fit(count_train,Y_train)
#predict on train
train_preds5 = KNN.predict(count_train)
#accuracy on train
print("Model accuracy on train is: ", accuracy_score(Y_train, train_preds5))

#predict on test
test_preds5 = KNN.predict(count_test)
#accuracy on test
print("Model accuracy on test is: ", accuracy_score(Y_test, test_preds5))
print('-'*50)

#Confusion matrix
print("confusion_matrix train is: ", metrics.confusion_matrix(Y_train, train_preds5))
print("confusion_matrix test is: ", metrics.confusion_matrix(Y_test, test_preds5))
print('Wrong predictions out of total')
print('-'*50)

# Wrong Predictions made.
print((Y_test !=test_preds5).sum(),'/',((Y_test == test_preds5).sum()+(Y_test != test_preds5).sum()))

print('-'*50)

print("# --- Code to Save the KNeighborsClassifier Model using pickle ---")
model_filename_nb = 'multinomial_KN_CV_model.pkl'
with open(model_filename_nb, 'wb') as file: # 'wb' means write binary
    pickle.dump(clf, file)
print(f"\nMultinomialNB model saved successfully to {model_filename_nb}")

print("CODE EXECUTION COMPLETED")